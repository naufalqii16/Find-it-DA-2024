{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import library and initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.impute import KNNImputer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('train_features.csv')\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['jumlah_promosi'] = train_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_features.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['tanggal_menjadi_anggota'], axis=1)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "for i in cat_cols:\n",
    "    print(f'{df[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pendidikan'] = df['pendidikan'].replace('5', np.nan)\n",
    "df['status_pernikahan'] = df['status_pernikahan'].replace('5', np.nan)\n",
    "\n",
    "for i in cat_cols:\n",
    "    print(f'{df[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_statistics(df):\n",
    "    \n",
    "    # Create a DataFrame to show column information\n",
    "\n",
    "    null_info = pd.DataFrame({\n",
    "        'Column Name': df.columns,\n",
    "        'Column Data Type': df.dtypes,\n",
    "        'Null Values': df.isnull().sum(),\n",
    "        'Null Percentage': df.isnull().mean() * 100\n",
    "    })\n",
    "\n",
    "    null_info.reset_index(drop=True, inplace=True)  # Reset the index\n",
    "\n",
    "    # Display the DataFrame\n",
    "    \n",
    "    null_info = null_info[null_info[\"Null Values\"] > 0]\n",
    "    null_info = null_info.sort_values(by = 'Null Values', ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"There are {null_info.shape[0]} columns ({null_info.shape[0] / len(df.columns) * 100:,.2f} %) with Null values out of {len(df.columns)} columns in Dataframe.\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return null_info\n",
    "\n",
    "train_null_data = null_statistics(df)\n",
    "\n",
    "train_null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_handled = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_with_knn(data):\n",
    "    num_columns = data.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "    num_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    num_imputed_data = num_imputer.fit_transform(data[num_columns])\n",
    "    num_imputed_byKNN_df = pd.DataFrame(num_imputed_data, columns=num_columns)\n",
    "    return num_imputed_byKNN_df\n",
    "\n",
    "num_imputed_byKNN_df = impute_with_knn(df_null_handled)\n",
    "\n",
    "num_imputed_byKNN_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_cat_with_mode(data):\n",
    "    cat_columns = data.select_dtypes(['object']).columns\n",
    "\n",
    "    for col in cat_columns:\n",
    "        data[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return data[cat_columns]\n",
    "\n",
    "df_null_handled['pendidikan', 'status_pernikahan'] = impute_cat_with_mode(df_null_handled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df null handled by mode and knn = df_mk\n",
    "num_columns = df_null_handled.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "df_null_handled[num_columns] = num_imputed_byKNN_df\n",
    "df_mk = df_null_handled.copy()\n",
    "\n",
    "df_mk.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Values Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns from df_mk\n",
    "\n",
    "numerical_columns = df_mk.select_dtypes(include='number')\n",
    "\n",
    "# Sort columns by correlation with 'SalePrice' in ascending order\n",
    "\n",
    "sorted_columns = numerical_columns.columns\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "\n",
    "num_plots = len(sorted_columns)\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "\n",
    "num_cols = 3\n",
    "num_rows = (num_plots - 1) // num_cols + 1\n",
    "\n",
    "# Create subplots\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "\n",
    "# Add a main title to the entire set of subplots\n",
    "\n",
    "fig.suptitle(\"Box Plots For Original Numerical Features\", y=1, fontsize=20)\n",
    "\n",
    "# Create a box plot for each feature\n",
    "\n",
    "for i, col in enumerate(sorted_columns):\n",
    "    \n",
    "    # Calculate current row and column for the subplot\n",
    "    \n",
    "    row_idx = i // num_cols\n",
    "    col_idx = i % num_cols\n",
    "    \n",
    "    # Calculate and add information about outliers\n",
    "\n",
    "    Q1 = df_mk[col].quantile(0.25)\n",
    "    Q3 = df_mk[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = (df_mk[col] < Q1 - 1.5 * IQR) | (df_mk[col] > Q3 + 1.5 * IQR)\n",
    "    percent_iqr_outliers = (sum(iqr_outliers) / len(df_mk)) * 100\n",
    "    \n",
    "    sns.boxplot(x=df_mk[col], ax=axes[row_idx, col_idx])\n",
    "    axes[row_idx, col_idx].set_xlabel(col, fontsize=12)\n",
    "    axes[row_idx, col_idx].set_title(f\"\\n\"\n",
    "                                     f'Box Plot for {col}\\n'\n",
    "                                     f\"\\n\"\n",
    "                                     f'Outliers: {sum(iqr_outliers)} ({percent_iqr_outliers:.2f}%)\\n', fontsize=14, color='red')\n",
    "    \n",
    "    axes[row_idx, col_idx].grid(True)\n",
    "\n",
    "# Remove any empty subplots\n",
    "\n",
    "for i in range(num_plots, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Adjust layout\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pada kolom tahun_kelahiran, hapus outlier karena agaknya aneh jika terdapat orang di data tersebut yang lahir pada 100 tahun yang lalu\n",
    "\n",
    "Q1 = df_mk['tahun_kelahiran'].quantile(0.25)\n",
    "Q3 = df_mk['tahun_kelahiran'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Tentukan batas bawah dan batas atas untuk outlier\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Hapus baris dengan nilai tahun_kelahiran di luar batas bawah dan batas atas\n",
    "df_mk_filtered = df_mk[(df_mk['tahun_kelahiran'] >= lower_bound) & (df_mk['tahun_kelahiran'] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns from df_mk_filtered\n",
    "\n",
    "numerical_columns = df_mk_filtered.select_dtypes(include='number')\n",
    "\n",
    "# Sort columns by correlation with 'SalePrice' in ascending order\n",
    "\n",
    "sorted_columns = numerical_columns.columns\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "\n",
    "num_plots = len(sorted_columns)\n",
    "\n",
    "# Calculate the number of rows and columns for the subplots\n",
    "\n",
    "num_cols = 3\n",
    "num_rows = (num_plots - 1) // num_cols + 1\n",
    "\n",
    "# Create subplots\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "\n",
    "# Add a main title to the entire set of subplots\n",
    "\n",
    "fig.suptitle(\"Box Plots For Original Numerical Features\", y=1, fontsize=20)\n",
    "\n",
    "# Create a box plot for each feature\n",
    "\n",
    "for i, col in enumerate(sorted_columns):\n",
    "    \n",
    "    # Calculate current row and column for the subplot\n",
    "    \n",
    "    row_idx = i // num_cols\n",
    "    col_idx = i % num_cols\n",
    "    \n",
    "    # Calculate and add information about outliers\n",
    "\n",
    "    Q1 = df_mk_filtered[col].quantile(0.25)\n",
    "    Q3 = df_mk_filtered[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_outliers = (df_mk_filtered[col] < Q1 - 1.5 * IQR) | (df_mk_filtered[col] > Q3 + 1.5 * IQR)\n",
    "    percent_iqr_outliers = (sum(iqr_outliers) / len(df_mk_filtered)) * 100\n",
    "    \n",
    "    sns.boxplot(x=df_mk_filtered[col], ax=axes[row_idx, col_idx])\n",
    "    axes[row_idx, col_idx].set_xlabel(col, fontsize=12)\n",
    "    axes[row_idx, col_idx].set_title(f\"\\n\"\n",
    "                                     f'Box Plot for {col}\\n'\n",
    "                                     f\"\\n\"\n",
    "                                     f'Outliers: {sum(iqr_outliers)} ({percent_iqr_outliers:.2f}%)\\n', fontsize=14, color='red')\n",
    "    \n",
    "    axes[row_idx, col_idx].grid(True)\n",
    "\n",
    "# Remove any empty subplots\n",
    "\n",
    "for i in range(num_plots, num_rows * num_cols):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Adjust layout\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Possible Wrong Inputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk_filtered['jumlah_anak_balita'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "terdapat jumlah anak yang continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pembulatan input yang salah\n",
    "def round_value(value):\n",
    "    if value % 1 < 0.5:\n",
    "        return int(np.floor(value))\n",
    "    else:\n",
    "        return int(np.ceil(value))\n",
    "    \n",
    "\n",
    "# visualize updated data\n",
    "def show_countplot(data):\n",
    "  possible_wrong_input_cols = data[['jumlah_anak_balita', 'jumlah_anak_remaja', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan', 'terakhir_belanja']]\n",
    "  num_cols = 2\n",
    "  num_rows = (possible_wrong_input_cols.shape[1] + num_cols -1 ) // num_cols\n",
    "\n",
    "  fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "\n",
    "  # Add a main title to the entire set of subplots\n",
    "  fig.suptitle(\"Countplot for outlier column in the dataset\", y=1, fontsize=20)\n",
    "\n",
    "\n",
    "  for i, col in enumerate(possible_wrong_input_cols):\n",
    "\n",
    "      # Calculate current row and column for the subplot\n",
    "      row_idx = i // num_cols\n",
    "      col_idx = i % num_cols\n",
    "\n",
    "      pic = sns.countplot(possible_wrong_input_cols, x=col, ax=axes[row_idx, col_idx])\n",
    "      axes[row_idx, col_idx].set_xlabel(col, fontsize=12)\n",
    "      pic.margins(x=0.1)\n",
    "\n",
    "  # Remove any empty subplots\n",
    "  for i in range(len(possible_wrong_input_cols.columns), num_rows * num_cols):\n",
    "      fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "  # Adjust layout\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "possible_wrong_input_cols = ['jumlah_anak_balita', 'jumlah_anak_remaja', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan', 'terakhir_belanja']\n",
    "\n",
    "for col in possible_wrong_input_cols:\n",
    "    df_mk_filtered[col] = df_mk_filtered[col].apply(lambda x: round_value(x))\n",
    "    \n",
    "show_countplot(df_mk_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df_mk_filtered.select_dtypes(include=['object']).columns\n",
    "for i in cat_cols:\n",
    "    print(f'{df_mk_filtered[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_custom_ordinal_encoding_mappings(df, default_value = 0):\n",
    "    \n",
    "    ordinal_encoding_columns_mappings = {\n",
    "    \n",
    "        'pendidikan': {'SMP': 1, 'SMA': 2, 'Sarjana': 3, 'Magister': 4, 'Doktor': 5},\n",
    "        'status_pernikahan': {'Sendiri': 1, 'Rencana Menikah': 2, 'Menikah': 3, 'Cerai': 4, 'Cerai Mati': 5}\n",
    "    }\n",
    "    \n",
    "    for col, mapping in ordinal_encoding_columns_mappings.items():\n",
    "        \n",
    "        if col in df.columns:\n",
    "            \n",
    "            df[col] = df[col].map(mapping).fillna(default_value)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk_filtered[cat_cols] = apply_custom_ordinal_encoding_mappings(df_mk_filtered[cat_cols])\n",
    "for i in cat_cols:\n",
    "    print(f'{df_mk_filtered[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk_filtered_desc = df_mk_filtered.describe().T\n",
    "df_mk_filtered_desc['skewness'] = df_mk_filtered.select_dtypes(include=[np.number]).skew()\n",
    "df_mk_filtered_desc['range'] = np.ptp(df_mk_filtered.select_dtypes(include=[np.number]), axis=0)\n",
    "df_mk_filtered_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(df_mk_filtered.corr(),annot=True, annot_kws={\"fontsize\": 8})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "feature_selected = ['tahun_kelahiran', 'pendidikan', 'status_pernikahan', 'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', 'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan','pembelian_diskon', 'belanja_kue', 'pembelian_web', 'pembelian_toko']\n",
    "feature_df = df_mk_filtered[feature_selected].copy()\n",
    "target_df = df_mk_filtered['jumlah_promosi'].copy()\n",
    "\n",
    "# Fit and transform features\n",
    "scaler = RobustScaler()\n",
    "scaled_features = scaler.fit_transform(feature_df)\n",
    "\n",
    "# Convert scaled features array to DataFrame\n",
    "scaled_feature_df = pd.DataFrame(scaled_features, columns=feature_selected)\n",
    "\n",
    "# Fit and transform target\n",
    "target_scaler = RobustScaler()\n",
    "scaled_target = target_scaler.fit_transform(target_df.values.reshape(-1, 1))\n",
    "\n",
    "# Convert scaled target array to DataFrame\n",
    "scaled_target_df = pd.DataFrame(scaled_target, columns=['jumlah_promosi'])\n",
    "\n",
    "scaled_feature_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_feature_df.copy()\n",
    "y = target_df.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "LogisticRegression = LogisticRegression()\n",
    "RidgeClassifier = RidgeClassifier()\n",
    "SGDClassifier = SGDClassifier(loss='log')  \n",
    "DecisionTreeClassifier = DecisionTreeClassifier()\n",
    "RandomForestClassifier = RandomForestClassifier()\n",
    "BaggingClassifier = BaggingClassifier()\n",
    "ExtraTreesClassifier = ExtraTreesClassifier()\n",
    "AdaBoostClassifier = AdaBoostClassifier()\n",
    "GradientBoostingClassifier = GradientBoostingClassifier()\n",
    "KNeighborsClassifier = KNeighborsClassifier()\n",
    "svc = SVC(probability=True) \n",
    "XGBClassifier = XGBClassifier()\n",
    "LGBMClassifier = LGBMClassifier()\n",
    "CatBoostClassifier = CatBoostClassifier(verbose=0)\n",
    "\n",
    "\n",
    "LogisticRegression = LogisticRegression.fit(X_train, y_train)\n",
    "RidgeClassifier = RidgeClassifier.fit(X_train, y_train)\n",
    "SGDClassifier = SGDClassifier.fit(X_train, y_train)\n",
    "DecisionTreeClassifier = DecisionTreeClassifier.fit(X_train, y_train)\n",
    "RandomForestClassifier = RandomForestClassifier.fit(X_train, y_train)\n",
    "BaggingClassifier = BaggingClassifier.fit(X_train, y_train)\n",
    "ExtraTreesClassifier = ExtraTreesClassifier.fit(X_train, y_train)\n",
    "AdaBoostClassifier = AdaBoostClassifier.fit(X_train, y_train)\n",
    "GradientBoostingClassifier = GradientBoostingClassifier.fit(X_train, y_train)\n",
    "KNeighborsClassifier = KNeighborsClassifier.fit(X_train, y_train)\n",
    "svc = svc.fit(X_train, y_train)\n",
    "XGBClassifier = XGBClassifier.fit(X_train, y_train)\n",
    "LGBMClassifier = LGBMClassifier.fit(X_train, y_train)\n",
    "CatBoostClassifier = CatBoostClassifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_logisticRegression = LogisticRegression.predict(X_test)\n",
    "y_pred_RidgeClassifier = RidgeClassifier.predict(X_test)\n",
    "y_pred_SGDClassifier = SGDClassifier.predict(X_test)\n",
    "y_pred_DecisionTreeClassifier = DecisionTreeClassifier.predict(X_test)\n",
    "y_pred_RandomForestClassifier = RandomForestClassifier.predict(X_test)\n",
    "y_pred_BaggingClassifier = BaggingClassifier.predict(X_test)\n",
    "y_pred_ExtraTreesClassifier = ExtraTreesClassifier.predict(X_test)\n",
    "y_pred_AdaBoostClassifier = AdaBoostClassifier.predict(X_test)\n",
    "y_pred_GradientBoostingClassifier = GradientBoostingClassifier.predict(X_test)\n",
    "y_pred_KNeighborsClassifier = KNeighborsClassifier.predict(X_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_XGBClassifier = XGBClassifier.predict(X_test)\n",
    "y_pred_LGBMClassifier = LGBMClassifier.predict(X_test)\n",
    "y_pred_CatBoostClassifier = CatBoostClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [y_pred_SGDClassifier, y_pred_DecisionTreeClassifier, y_pred_RandomForestClassifier, y_pred_BaggingClassifier, y_pred_ExtraTreesClassifier, y_pred_AdaBoostClassifier, y_pred_GradientBoostingClassifier, y_pred_KNeighborsClassifier, y_pred_svc, y_pred_XGBClassifier, y_pred_LGBMClassifier, y_pred_CatBoostClassifier]\n",
    "model_names = ['SGDClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier', 'BaggingClassifier', 'ExtraTreesClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'svc', 'XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier']\n",
    "\n",
    "# Metrik evaluasi yang akan digunakan\n",
    "metrics = {\n",
    "    'Model Name' : [],\n",
    "    'F1 Score': []\n",
    "}\n",
    "\n",
    "# Looping untuk mengevaluasi setiap model\n",
    "for i in range(len(model_names)):\n",
    "    \n",
    "    metrics['Model Name'].append(model_names[i])\n",
    "    metrics['F1 Score'].append(f1_score(y_test, predictions[i], average='macro'))\n",
    "\n",
    "metrics = pd.DataFrame(metrics)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test_features.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['tanggal_menjadi_anggota'], axis=1)\n",
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df_test_test.select_dtypes(include=['object']).columns\n",
    "for i in cat_cols:\n",
    "    print(f'{df_test[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['pendidikan'] = df_test['pendidikan'].replace('5', np.nan)\n",
    "df_test['status_pernikahan'] = df_test['status_pernikahan'].replace('5', np.nan)\n",
    "\n",
    "for i in cat_cols:\n",
    "    print(f'{df_test[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null_data = null_statistics(df_test)\n",
    "test_null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_null_handled = df_test.copy()\n",
    "\n",
    "num_test_imputed_byKNN_df = impute_with_knn(df_test_null_handled)\n",
    "\n",
    "num_test_imputed_byKNN_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_null_handled['pendidikan', 'status_pernikahan'] = impute_cat_with_mode(df_test_null_handled)\n",
    "\n",
    "# df null handled by mode and knn = df_mk\n",
    "num_columns = df_test_null_handled.select_dtypes(['int64', 'float64']).columns\n",
    "\n",
    "df_test_null_handled[num_columns] = num_imputed_byKNN_df\n",
    "df_test_mk = df_test_null_handled.copy()\n",
    "\n",
    "df_test_mk.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_wrong_input_cols = ['jumlah_anak_balita', 'jumlah_anak_remaja', 'pembelian_diskon', 'pembelian_web', 'pembelian_toko', 'keluhan', 'terakhir_belanja']\n",
    "\n",
    "for col in possible_wrong_input_cols:\n",
    "    df_test_mk[col] = df_test_mk[col].apply(lambda x: round_value(x))\n",
    "    \n",
    "show_countplot(df_test_mk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mk[cat_cols] = apply_custom_ordinal_encoding_mappings(df_test_mk[cat_cols])\n",
    "for i in cat_cols:\n",
    "    print(f'{df_test_mk[i].value_counts()}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selected = ['tahun_kelahiran', 'pendidikan', 'status_pernikahan', 'pendapatan', 'jumlah_anak_balita', 'jumlah_anak_remaja', 'terakhir_belanja', 'belanja_buah', 'belanja_daging', 'belanja_ikan','pembelian_diskon', 'belanja_kue', 'pembelian_web', 'pembelian_toko']\n",
    "feature_df_test = df_test_mk[feature_selected].copy()\n",
    "\n",
    "# Fit and transform features\n",
    "scaler_test = RobustScaler()\n",
    "scaled_features_test = scaler_test.fit_transform(feature_df_test)\n",
    "\n",
    "# Convert scaled features array to DataFrame\n",
    "scaled_feature_df_test = pd.DataFrame(scaled_features_test, columns=feature_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaled_feature_df.copy()\n",
    "y_train = target_df.copy()\n",
    "X_test = scaled_feature_df_test.copy\n",
    "\n",
    "\n",
    "RandomForestClassifier = RandomForestClassifier()\n",
    "ExtraTreesClassifier = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "\n",
    "RandomForestClassifier = RandomForestClassifier.fit(X_train, y_train)\n",
    "ExtraTreesClassifier = ExtraTreesClassifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_RandomForestClassifier = RandomForestClassifier.predict(X_test)\n",
    "y_pred_ExtraTreesClassifier = ExtraTreesClassifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.DataFrame(y_pred_RandomForestClassifier)\n",
    "etc_df = pd.DataFrame(y_pred_ExtraTreesClassifier)\n",
    "\n",
    "rf_df.to_csv('rf.csv')\n",
    "etc_df.to_csv('etc.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
